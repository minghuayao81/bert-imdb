{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ada9c82e-c9b1-47c3-8bb8-6571ed27933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: datasets in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: torch in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (1.23.5)\n",
      "Requirement already satisfied: filelock in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: networkx in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch pandas scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790a842-2625-480d-9539-3c23d523af7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/Users/minghuayao/+++AutoDL/models/bert-base-uncased/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/Users/minghuayao/+++AutoDL/models/bert-base-uncased/' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 初始化tokenizer\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOLD_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_preprocess_data\u001b[39m():\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# 示例数据 - 实际应替换为真实数据加载\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# 假设原始DataFrame包含text、split、label三列\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is positive example.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative sentence here.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnother neutral text.\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     45\u001b[0m     }\n",
      "File \u001b[0;32m~/Dev/brew/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:637\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Dev/brew/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1761\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1756\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1757\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1758\u001b[0m     )\n\u001b[1;32m   1760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1762\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1764\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1765\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1766\u001b[0m     )\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/Users/minghuayao/+++AutoDL/models/bert-base-uncased/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/Users/minghuayao/+++AutoDL/models/bert-base-uncased/' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "# 环境要求：Python 3.8+, PyTorch 2.0+, Transformers 4.30+\n",
    "# 安装依赖：pip install transformers datasets torch pandas scikit-learn\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "# from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 参数配置\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 32  # 4090显存充足可以适当增大\n",
    "EPOCHS = 3\n",
    "OLD_MODEL_PATH = \"/Users/minghuayao/+++AutoDL/models/bert-base-uncased/\"\n",
    "NEW_MODEL_PATH = \"/Users/minghuayao/+++AutoDL/models/new/\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import os\n",
    "from datasets import DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 初始化tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(OLD_MODEL_PATH+\"tokenizer\")\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "\n",
    "    # 示例数据 - 实际应替换为真实数据加载\n",
    "    # 假设原始DataFrame包含text、split、label三列\n",
    "    data = {\n",
    "        \"text\": [\"This is positive example.\", \"Negative sentence here.\", \"Another neutral text.\"],\n",
    "        \"split\": [\"train\", \"train\", \"test\"],\n",
    "        \"label\": [1, 0, 2]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # 创建训练测试分割\n",
    "    train_df = df[df[\"split\"] == \"train\"]\n",
    "    \n",
    "    # 创建DatasetDict并彻底清除索引\n",
    "    final_dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_df, preserve_index=False),\n",
    "        \"test\": Dataset.from_pandas(\n",
    "            df[df[\"split\"] == \"test\"], \n",
    "            preserve_index=False\n",
    "        )\n",
    "    })\n",
    "\n",
    "    # 单步处理函数\n",
    "    def process_batch(examples):\n",
    "        # 分词处理\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_token_type_ids=False  # 可选：是否返回token类型ID\n",
    "        )\n",
    "        \n",
    "        # 直接返回结构化数据\n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": examples[\"label\"]\n",
    "        }\n",
    "\n",
    "    # 应用处理并清除所有原始列\n",
    "    tokenized_dataset = final_dataset.map(\n",
    "        process_batch,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\", \"split\", \"label\"],  # 仅删除确切存在的列\n",
    "        desc=\"Processing text data\"\n",
    "    )\n",
    "    \n",
    "    # 显式设置张量格式\n",
    "    tokenized_dataset.set_format(\n",
    "        \"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    )\n",
    "\n",
    "    # ==== 新增部分 ====\n",
    "    # 收集所有标签值并创建映射\n",
    "    all_labels = (\n",
    "        final_dataset[\"train\"][\"label\"] \n",
    "        + final_dataset[\"test\"][\"label\"]\n",
    "    )\n",
    "    unique_labels = sorted(np.unique(all_labels))\n",
    "    label2id = {orig: new for new, orig in enumerate(unique_labels)}\n",
    "    \n",
    "    def process_batch(examples):\n",
    "        tokenized = tokenizer(examples[\"text\"], ...)\n",
    "        \n",
    "        # 转换原始标签为从0开始\n",
    "        converted_labels = [label2id[lbl] for lbl in examples[\"label\"]]\n",
    "        \n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": converted_labels  # 使用转换后的标签\n",
    "        }\n",
    "    \n",
    "    return tokenized_dataset, label2id\n",
    "\n",
    "\n",
    "def create_model(num_labels):\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        OLD_MODEL_PATH,\n",
    "        num_labels=num_labels,  # 动态设置分类数量\n",
    "        id2label={v: str(v) for v in range(num_labels)}\n",
    "    )\n",
    "    return AutoModelForSequenceClassification.from_pretrained(OLD_MODEL_PATH, config=config)\n",
    "\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, train_loader, val_loader):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                #outputs = model(**batch)\n",
    "                # 应改为显式参数传递：\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(val_labels, val_preds)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train loss: {avg_train_loss:.4f}, Val accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "    print(f\"Training complete. Best validation accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59846eb-784a-409d-99ac-5e5ade148bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.serialization import safe_globals, add_safe_globals\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "add_safe_globals([\n",
    "    (np, 'dtype'),  # 允许dtype的构造\n",
    "    (np.core.multiarray, '_reconstruct'), \n",
    "    (np, '_DType'),  # numpy的dtype元类型\n",
    "])\n",
    "\n",
    "def load_safe_model(path):\n",
    "    # 加载检查点时强制使用安全模式\n",
    "    checkpoint = torch.load(path, map_location='cpu', weights_only=True)\n",
    "\n",
    "    # 重建配置参数（在运行时动态恢复dtype）\n",
    "    config_params = checkpoint['config'].copy()\n",
    "    if 'torch_dtype' in checkpoint['dtype_mapping']:\n",
    "        config_params['torch_dtype'] = getattr(\n",
    "            torch, \n",
    "            checkpoint['dtype_mapping']['torch_dtype']\n",
    "        )\n",
    "    \n",
    "    # 使用transformers的安全配置接口\n",
    "    config = AutoConfig.for_model(\n",
    "        **config_params\n",
    "    )\n",
    "    config.num_labels = checkpoint[\"num_labels\"]\n",
    "    \n",
    "    # 实例化并加载模型\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(new_model_path, text):\n",
    "    \"\"\"预测时应明确分割模型目录和tokenizer目录\"\"\" \n",
    "    model_dir = os.path.dirname(new_model_path)  # 提取目录路径\n",
    "    model_file = os.path.basename(new_model_path)\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        new_model_path,\n",
    "        map_location=DEVICE,\n",
    "        weights_only=True\n",
    "    )\n",
    "    \n",
    "    # 独立重建关键参数\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        new_model_path,\n",
    "        **{\n",
    "            k: v for k, v in checkpoint[\"config\"].items()\n",
    "            if not k.startswith('_')  # 规避内部参数\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 特殊处理分类头参数\n",
    "    config.num_labels = checkpoint[\"num_labels\"]\n",
    "    config.id2label = {i: str(i) for i in range(config.num_labels)}\n",
    "\n",
    "    # 实例化模型\n",
    "    model = load_safe_model(new_model_path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    # 编码与预测\n",
    "    tokenizer_path = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = outputs.logits.argmax().item()\n",
    "    \n",
    "    # 反向标签映射\n",
    "    id2label = {v: k for k, v in checkpoint[\"label_map\"].items()}\n",
    "    return id2label[pred_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561991a-3c16-4c97-8ec7-321c86f943f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text data: 100%|██████████| 2/2 [00:00<00:00, 33.19 examples/s]\n",
      "Processing text data: 100%|██████████| 1/1 [00:00<00:00, 437.50 examples/s]\n",
      "Some weights of the model checkpoint at /Users/minghuayao/+++AutoDL/models/bert-base-uncased/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /Users/minghuayao/+++AutoDL/models/bert-base-uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/minghuayao/Dev/brew/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train loss: 1.4785, Val accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "Train loss: 1.2819, Val accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "Train loss: 1.3463, Val accuracy: 1.0000\n",
      "Training complete. Best validation accuracy: 1.0000\n",
      "模型与tokenizer已保存至 /Users/minghuayao/+++AutoDL/models/bert-base-uncased/\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "from pathlib import Path\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":    \n",
    "    dataset, label_map = load_and_preprocess_data()\n",
    "    actual_labels = len(label_map)\n",
    "\n",
    "    # 创建自定义collate函数\n",
    "    def custom_collate(batch):\n",
    "        # 自动转换为张量并处理嵌套字典\n",
    "        processed_batch = {}\n",
    "        for key in batch[0].keys():\n",
    "            if key == \"label\":\n",
    "                processed_batch[key] = torch.stack([torch.tensor(item[key]) for item in batch])\n",
    "            else:\n",
    "                processed_batch[key] = default_collate([item[key] for item in batch])\n",
    "        return processed_batch    \n",
    "    \n",
    "    # 使用修正的collate_fn创建DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        dataset[\"train\"],\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset[\"test\"],\n",
    "        batch_size=32,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "\n",
    "    # 初始化模型\n",
    "    model = create_model(num_labels=actual_labels)\n",
    "\n",
    "    # 训练模型\n",
    "    train_model(model, train_loader, val_loader)\n",
    "\n",
    "    def deep_convert_dtypes(obj):\n",
    "        \"\"\"彻底转换所有层级的dtype为字符串\"\"\"\n",
    "        if isinstance(obj, np.dtype):\n",
    "            return str(obj)  # 转换为标准字符串表示如 'int64'\n",
    "        elif isinstance(obj, np.generic):\n",
    "            return obj.item()  # numpy标量转Python类型\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: deep_convert_dtypes(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return type(obj)(deep_convert_dtypes(v) for v in obj)\n",
    "        return obj\n",
    "\n",
    "    # 处理模型配置的每个参数\n",
    "    original_config = model.config.to_dict()\n",
    "    sanitized_config = deep_convert_dtypes(original_config)\n",
    "\n",
    "    # 添加二次验证（确保没有遗留dtype）\n",
    "    assert not any(isinstance(v, np.dtype) for v in sanitized_config.values()), \"发现未转换的dtype\"\n",
    "\n",
    "    # 保存检查点时强制所有数值类型为原生类型\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": {\n",
    "            k: int(v) if isinstance(v, np.integer) else \n",
    "            float(v) if isinstance(v, np.floating) else\n",
    "            v \n",
    "            for k, v in sanitized_config.items()\n",
    "        },\n",
    "        \"num_labels\": int(len(label_map)),\n",
    "        \"dtype_mapping\": {  # 显式记录dtype转换关系\n",
    "            \"torch_dtype\": \"int64\"  \n",
    "        }\n",
    "    }\n",
    "\n",
    "    # ====== 模型保存时需同时保存tokenizer ======\n",
    "    def save_model_and_tokenizer(model, tokenizer, new_model_path):\n",
    "        \"\"\"完整的模型保存函数\"\"\"\n",
    "        # 创建目录\n",
    "        os.makedirs(new_model_path, exist_ok=True)\n",
    "        \n",
    "        # 保存模型检查点\n",
    "        torch.save(checkpoint, os.path.join(new_model_path, \"final_bert_imdb_model.pt\"))\n",
    "        \n",
    "        # 保存tokenizer到独立的子目录 (关键步骤)\n",
    "        tokenizer.save_pretrained(new_model_path+\"tokenizer\")\n",
    "        \n",
    "        print(f\"模型与tokenizer已保存至 {new_model_path}\")\n",
    "\n",
    "    save_model_and_tokenizer(model, tokenizer, NEW_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07773613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载成功!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 测试加载\n",
    "model = load_safe_model(NEW_MODEL_PATH+\"final_bert_imdb_model.pt\")\n",
    "print(\"模型加载成功!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209748d2-caac-499f-a286-82b042de1d4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 使用样例\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_bert_imdb_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis movie is absolutely wonderful!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[102], line 80\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model_path, text)\u001b[0m\n\u001b[1;32m     77\u001b[0m     pred_id \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# 反向标签映射\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m id2label \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m id2label[pred_id]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label_map'"
     ]
    }
   ],
   "source": [
    "# 使用样例\n",
    "print(predict(NEW_MODEL_PATH+\"final_bert_imdb_model.pt\", \"This movie is absolutely wonderful!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed0ac3-7f2a-4a3d-bdc0-ed404a1ae909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fcf013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
